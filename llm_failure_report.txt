
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘             ğŸ” SAKHI LLM FAILURE DIAGNOSIS REPORT                            â•‘
â•‘                    Generated: 2025-12-29 11:06:03                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

================================================================================
ğŸš¨ CRITICAL FAILURE POINTS IDENTIFIED
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ï¸âƒ£  ROUTING FAILURES (model_gateway.py)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Location: modules/model_gateway.py - decide_route() method                   â”‚
â”‚                                                                              â”‚
â”‚ PROBLEMS:                                                                    â”‚
â”‚ â€¢ Lines 334-337: Medical complex always wins over medical simple             â”‚
â”‚   if medical_complex_sim >= medical_simple_sim â†’ OPENAI_RAG                  â”‚
â”‚   This can route SIMPLE queries to the slower/expensive OpenAI path.         â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 329-331: Facility queries routed to SLM_RAG BEFORE checking if       â”‚
â”‚   the query is actually about medical topics.                                â”‚
â”‚   "what is the cost of IVF in hyderabad clinic" could be mislabeled.         â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 246-248: Thresholds may be too aggressive:                           â”‚
â”‚   - SMALL_TALK_THRESHOLD = 0.75  (too high - misses casual variants)         â”‚
â”‚   - FACILITY_INFO_THRESHOLD = 0.50 (too low - false positives)               â”‚
â”‚                                                                              â”‚
â”‚ IMPACT: Wrong model selected â†’ irrelevant or poor quality responses          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2ï¸âƒ£  CLASSIFICATION FAILURES (response_builder.py)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Location: modules/response_builder.py - classify_message() function          â”‚
â”‚                                                                              â”‚
â”‚ PROBLEMS:                                                                    â”‚
â”‚ â€¢ Lines 44-75: SIGNAL classification uses simple text parsing                â”‚
â”‚   - Only checks for "YES" or "NO" in LLM output                              â”‚
â”‚   - LLM could return "Maybe", "Partially", or malformed output               â”‚
â”‚   - Default to "NO" if parsing fails (Line 74)                               â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 61-70: Fragile parsing logic:                                        â”‚
â”‚   if low.startswith("identified language"):  # Case sensitive issue          â”‚
â”‚   elif "[signal]" in low:  # Depends on exact formatting                     â”‚
â”‚                                                                              â”‚
â”‚ IMPACT: Medical questions classified as small talk â†’ no RAG retrieval        â”‚
â”‚ EXAMPLE: "à°¨à°¾à°•à± PCOS à°‰à°‚à°¦à°¿" (I have PCOS) might fail signal detection           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3ï¸âƒ£  RAG RETRIEVAL FAILURES (search_hierarchical.py)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Location: search_hierarchical.py - hierarchical_rag_query() function         â”‚
â”‚                                                                              â”‚
â”‚ PROBLEMS:                                                                    â”‚
â”‚ â€¢ Line 5: match_threshold = 0.3 is very low                                  â”‚
â”‚   - Low quality matches may be included                                      â”‚
â”‚   - Irrelevant content passed to LLM                                         â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 28-35: Silent failure mode                                           â”‚
â”‚   - Exception caught but only printed, not logged properly                   â”‚
â”‚   - Returns empty results on DB error                                        â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 45-58: FAQ search separate from doc search                           â”‚
â”‚   - YouTube link prioritized even if FAQ answer is irrelevant                â”‚
â”‚   - Mixing sources without clear relevance ranking                           â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Line 69: Returns "No relevant information found." for empty results        â”‚
â”‚   - LLM then hallucinates instead of admitting lack of knowledge             â”‚
â”‚                                                                              â”‚
â”‚ IMPACT: Poor context â†’ LLM generates inaccurate or hallucinated responses    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4ï¸âƒ£  SLM CLIENT FAILURES (slm_client.py)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Location: modules/slm_client.py - generate_chat() & generate_rag_response()  â”‚
â”‚                                                                              â”‚
â”‚ PROBLEMS:                                                                    â”‚
â”‚ â€¢ Lines 70-117: API call can fail silently then fallback to mock             â”‚
â”‚   - If SLM endpoint is slow, 30s timeout might not be enough                 â”‚
â”‚   - No retry mechanism for transient failures                                â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 75-78: Payload format mismatch                                       â”‚
â”‚   - "question" vs "message" field naming                                     â”‚
â”‚   - "chat_history": "" always empty (conversation context lost!)             â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 162-166: RAG context passed but might be ignored                     â”‚
â”‚   - "context" field may not be used by SLM endpoint                          â”‚
â”‚   - No verification that SLM actually uses the context                       â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 119-132: Mock responses leak into production                         â”‚
â”‚   - If SLM_ENDPOINT_URL not set, mock responses returned                     â”‚
â”‚   - "This is a mock SLM response" - BAD UX!                                  â”‚
â”‚                                                                              â”‚
â”‚ IMPACT: SLM returns generic/mock responses instead of contextual answers     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5ï¸âƒ£  RESPONSE GENERATION FAILURES (response_builder.py)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Location: modules/response_builder.py - generate_medical_response()          â”‚
â”‚                                                                              â”‚
â”‚ PROBLEMS:                                                                    â”‚
â”‚ â€¢ Lines 179-228: System prompt is VERY long (~3KB)                           â”‚
â”‚   - Complex formatting rules for follow-ups                                  â”‚
â”‚   - LLM often ignores complex instructions                                   â”‚
â”‚   - "MANDATORY RESPONSE STRUCTURE" is frequently violated                    â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 191-210: Strict follow-up format requirements                        â”‚
â”‚   - " Follow ups : " with specific spacing                                   â”‚
â”‚   - "Each question should be under 65 characters"                            â”‚
â”‚   - LLM frequently fails these exact requirements                            â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 218-228: Context injection at end of system prompt                   â”‚
â”‚   - "No KB retrieved" case gives vague guidance                              â”‚
â”‚   - LLM may hallucinate when context is empty                                â”‚
â”‚                                                                              â”‚
â”‚ â€¢ Lines 241-242: Response truncation to 1024 chars                           â”‚
â”‚   - May cut off mid-sentence or mid-follow-up                                â”‚
â”‚   - Could result in broken or incomplete responses                           â”‚
â”‚                                                                              â”‚
â”‚ IMPACT: Malformed responses, missing follow-ups, cut-off answers             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6ï¸âƒ£  LANGUAGE HANDLING FAILURES (main.py + response_builder.py)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Location: Multiple files                                                     â”‚
â”‚                                                                              â”‚
â”‚ PROBLEMS:                                                                    â”‚
â”‚ â€¢ main.py Line 187: Uses classified language, not user preference            â”‚
â”‚   detected_lang = classification.get("language", req.language)               â”‚
â”‚   User might prefer Telugu but LLM detects Tinglish                          â”‚
â”‚                                                                              â”‚
â”‚ â€¢ response_builder.py Lines 128-130: Tinglish handling is vague              â”‚
â”‚   "write Telugu words using Roman letters" - ambiguous instruction           â”‚
â”‚                                                                              â”‚
â”‚ â€¢ No validation of language output from LLM                                  â”‚
â”‚   - LLM might respond in English despite target_lang  = "te"                 â”‚
â”‚                                                                              â”‚
â”‚ IMPACT: Responses in wrong language, confusing Tinglish transliterations     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7ï¸âƒ£  CONVERSATION HISTORY FAILURES (main.py)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Location: main.py - Line 200                                                 â”‚
â”‚                                                                              â”‚
â”‚ PROBLEMS:                                                                    â”‚
â”‚ â€¢ history = get_last_messages(user_id, limit=5)                              â”‚
â”‚   - Only 5 messages - may lose important context                             â”‚
â”‚   - History passed to OpenAI but NOT to SLM routes!                          â”‚
â”‚                                                                              â”‚
â”‚ â€¢ SLM routes (Lines 203-223 and 226-272) don't include history:              â”‚
â”‚   - slm_client.generate_chat() doesn't get conversation history              â”‚
â”‚   - slm_client.generate_rag_response() gets context but no history           â”‚
â”‚                                                                              â”‚
â”‚ IMPACT: SLM has no memory â†’ repetitive or context-blind responses            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

================================================================================
ğŸ“Š FAILURE FREQUENCY MATRIX (Estimated)
================================================================================

| Failure Type                  | Frequency | Severity | User Impact        |
|-------------------------------|-----------|----------|-------------------|
| Wrong routing decision        | HIGH      | MEDIUM   | Slow/poor answers |
| RAG returns no context        | MEDIUM    | HIGH     | Hallucinated info |
| SLM mock responses            | LOW       | CRITICAL | Broken UX         |
| Classification fails          | MEDIUM    | MEDIUM   | Wrong mode        |
| Response truncation           | MEDIUM    | LOW      | Incomplete answer |
| Language mismatch             | HIGH      | MEDIUM   | User confusion    |
| Missing conversation context  | HIGH      | HIGH     | Repetitive Qs     |
| Follow-up format broken       | HIGH      | LOW      | Bad UX            |

================================================================================
ğŸ› ï¸  RECOMMENDED FIXES (Priority Order)
================================================================================

1ï¸âƒ£  FIX CONVERSATION HISTORY FOR SLM (HIGH PRIORITY)
   Location: modules/slm_client.py
   Change: Pass chat_history to SLM payload instead of empty string
   Status: âŒ Not implemented

2ï¸âƒ£  ADD RAG FALLBACK CHAIN (HIGH PRIORITY)
   Location: main.py Lines 226-272
   Change: If SLM_RAG returns poor answer, fallback to OPENAI_RAG
   Status: âŒ Not implemented

3ï¸âƒ£  IMPROVE ROUTING THRESHOLDS (MEDIUM PRIORITY)
   Location: modules/model_gateway.py
   Change: Lower SMALL_TALK_THRESHOLD to 0.70, raise FACILITY to 0.60
   Status: âŒ Not implemented

4ï¸âƒ£  ADD RESPONSE VALIDATION (MEDIUM PRIORITY)
   Location: modules/response_builder.py
   Change: Validate response format before returning, fix malformed output
   Status: âŒ Not implemented

5ï¸âƒ£  REMOVE MOCK RESPONSES FROM PRODUCTION (HIGH PRIORITY)
   Location: modules/slm_client.py
   Change: Raise exception instead of returning mock response
   Status: âŒ Not implemented

6ï¸âƒ£  IMPROVE RAG THRESHOLD (LOW PRIORITY)
   Location: search_hierarchical.py
   Change: Increase match_threshold to 0.5 for higher quality matches
   Status: âŒ Not implemented

================================================================================
ğŸ§ª TEST QUERIES TO VERIFY FAILURES
================================================================================

Run these queries through terminal_chat.py to observe failures:

1. ROUTING TEST - Should use SLM but might use OpenAI:
   "what is ivf"
   
2. LANGUAGE TEST - Should respond in Telugu:
   "IVF ante enti?" (What is IVF?)
   
3. CONTEXT TEST - Should remember previous answer:
   "What is the cost of IVF?"
   â†’ then ask: "And what about in Hyderabad?"
   
4. RAG TEST - Should return accurate info:
   "What are the symptoms of PCOS?"
   
5. FACILITY TEST - Should give clinic info:
   "Where is the Vizag clinic?"
   
6. SMALL TALK TEST - Should be warm, not medical:
   "I'm feeling sad today"

================================================================================
ğŸ“ FILES REQUIRING MODIFICATIONS
================================================================================

Priority 1 (Critical):
  â†’ modules/slm_client.py (Add history, remove mock)
  â†’ main.py (Pass history to SLM routes)

Priority 2 (Important):
  â†’ modules/model_gateway.py (Tune thresholds)
  â†’ search_hierarchical.py (Improve RAG quality)

Priority 3 (Nice to have):
  â†’ modules/response_builder.py (Simplify prompts)
  â†’ modules/text_utils.py (Smarter truncation)

================================================================================
